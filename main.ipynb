{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cookie Consent Management Solution Test\n",
    "#### This notebook demonstrates the complete flow of checking cookie consent management on websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from url_processor import URLProcessor\n",
    "from provider_registry import ProviderRegistry\n",
    "from browser_manager import BrowserManager\n",
    "from data_collection import DataCollectionService\n",
    "import json, visualisation\n",
    "from IPython.display import display, HTML\n",
    "from dataclasses import asdict\n",
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import graphviz_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_processor = URLProcessor()\n",
    "provider_registry = ProviderRegistry()\n",
    "browser_manager = BrowserManager(provider_registry)\n",
    "data_collector = DataCollectionService(browser_manager)\n",
    "print(\"All components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get URLs from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"Choose URL input method:\")\n",
    "print(\"1. Enter comma-separated URLs\")\n",
    "print(\"2. Use default Abbott URLs\")\n",
    "choice = input(\"Enter choice (1 or 2): \")\n",
    "\n",
    "if choice == \"1\":\n",
    "   urls_input = input(\"Enter URLs (comma-separated): \")\n",
    "   test_urls = [url.strip() for url in urls_input.split(\",\")]\n",
    "else:\n",
    "   test_urls = [\n",
    "       \"it.pediasure.abbott\",\n",
    "       \"it.ensure.abbott\", \n",
    "       \"es.ensure.abbott\",\n",
    "       \"es.pediasure.abbott\"\n",
    "   ]\n",
    "\n",
    "# Process URLs to ensure proper format\n",
    "formatted_urls = []\n",
    "for url in test_urls:\n",
    "   if not url.startswith(('http://', 'https://')):\n",
    "       url = 'https://' + url\n",
    "   formatted_urls.append(url)\n",
    "\n",
    "print(f\"\\nProcessing {len(formatted_urls)} URLs:\")\n",
    "for url in formatted_urls:\n",
    "   print(f\"- {url}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure URL format is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = ['it.ensure.abbott']\n",
    "formatted_urls = []\n",
    "for url in test_urls:\n",
    "   if not url.startswith(('http://', 'https://')):\n",
    "       url = 'https://' + url\n",
    "   formatted_urls.append(url)\n",
    "\n",
    "print(f\"\\nProcessing {len(formatted_urls)} URLs:\")\n",
    "for url in formatted_urls:\n",
    "   print(f\"- {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting URL processing...\")\n",
    "url_results = url_processor.process_urls(formatted_urls)\n",
    "print(f\"URL processing complete. {len(url_results)} results obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect consent data for each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for i, url_result in enumerate(url_results, 1):\n",
    "    print(\"Processing URL\")\n",
    "    if url_result.is_valid:\n",
    "        print(\"URL is valid, checking for cookie consent...\")\n",
    "        result = data_collector.create_result(url_result)\n",
    "        all_results.append(result)\n",
    "        print(\"Cookie consent check complete\")\n",
    "    else:\n",
    "        print(f\"Skipping invalid URL. Error: {url_result.error_message}\")\n",
    "    \n",
    "print(f\"Processed {len(all_results)} valid URLs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nGenerating results...\")\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"\\n=== Result {i}/{len(all_results)} ===\")\n",
    "    print(f\"URL: {result.url_info['requested_url']}\")\n",
    "    \n",
    "    #print(\"\\nFull Result:\")\n",
    "    #print(json.dumps(asdict(result), indent=2))\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    summary = data_collector.generate_cod_results(result, include_network_chains=True)\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    \n",
    "    if result.errors:\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        for error in result.errors:\n",
    "            print(f\"- {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes URLs by removing 'http://', 'https://', and 'www.' prefixes.\n",
    "    For example: 'https://www.example.com' becomes 'example.com'\n",
    "    \"\"\"\n",
    "    # Remove http:// or https://\n",
    "    if \"://\" in url:\n",
    "        url = url.split(\"://\", 1)[1]\n",
    "    \n",
    "    # Remove www.\n",
    "    if url.startswith(\"www.\"):\n",
    "        url = url[4:]\n",
    "        \n",
    "    return url\n",
    "\n",
    "def shorten_url(url):\n",
    "    \"\"\"\n",
    "    For example, 'https://abc.com/page/something' becomes 'abc.com/page'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First normalize the URL\n",
    "        url = normalize_url(url)\n",
    "        parsed = urlparse(f\"http://{url}\")  # Add scheme to make urlparse work correctly\n",
    "        netloc = parsed.netloc\n",
    "        # Process the path: strip leading/trailing slashes and split by '/'\n",
    "        if parsed.path and parsed.path != \"/\":\n",
    "            parts = parsed.path.strip(\"/\").split(\"/\")\n",
    "            if parts:\n",
    "                return f\"{netloc}/{parts[0]}\"\n",
    "        return netloc\n",
    "    except Exception:\n",
    "        # In case of any parsing error, return the original URL.\n",
    "        return url\n",
    "    \n",
    "def collapse_url(url: str, requested_url: str, max_path_sections: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Collapses URLs, with special handling for the requested_url:\n",
    "    - If URL starts with requested_url, return requested_url\n",
    "    - Otherwise collapse normally\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Normalize both URLs before comparison\n",
    "        normalized_url = normalize_url(url)\n",
    "        normalized_requested = normalize_url(requested_url)\n",
    "        \n",
    "        # Check if this URL is or starts with the requested_url\n",
    "        if normalized_url.startswith(normalized_requested):\n",
    "            return requested_url\n",
    "            \n",
    "        parsed = urlparse(f\"http://{normalized_url}\")  # Add scheme to make urlparse work correctly\n",
    "        netloc = parsed.netloc or \"unknown\"\n",
    "        # Split path into segments, ignoring empty parts\n",
    "        path_parts = [p for p in parsed.path.strip(\"/\").split(\"/\") if p]\n",
    "        # Keep only up to 'max_path_sections' segments\n",
    "        collapsed = \"/\".join(path_parts[:max_path_sections])\n",
    "        return f\"{netloc}/{collapsed}\" if collapsed else netloc\n",
    "    except Exception:\n",
    "        return url\n",
    "\n",
    "def get_node_color(short_label: str) -> str:\n",
    "    \"\"\"Assigns colors to nodes based on their domain.\"\"\"\n",
    "    lower_label = short_label.lower()\n",
    "    \n",
    "    # Mapping from keyword substring -> color\n",
    "    keyword_to_color = {\n",
    "        \"facebook\": \"blue\",\n",
    "        \"amazon\": \"orange\",\n",
    "        \"tiktok\": \"black\",\n",
    "        \"abbott\": \"royalblue\",\n",
    "        \"hubspot\": \"orange\",\n",
    "        \"hs-analytics\": \"orange\",\n",
    "        \"google\": \"yellow\",\n",
    "        \"googletagmanager\": \"yellow\",\n",
    "        \"doubleclick\": \"yellow\",\n",
    "        \"trustarc\": \"green\",\n",
    "    }\n",
    "    \n",
    "    # Return the first match or default to gray\n",
    "    for keyword, color in keyword_to_color.items():\n",
    "        if keyword in lower_label:\n",
    "            return color\n",
    "    return \"gray\"\n",
    "\n",
    "def draw_network_graph(result, hierarchical=False, collapse=False, make_url_short=False, phase=\"Pre-consent\"):\n",
    "    # Convert the dataclass to a dictionary for dictionary-style access\n",
    "    result_dict = asdict(result)\n",
    "    requested_url = result_dict[\"url_info\"][\"requested_url\"]\n",
    "\n",
    "    # Extract the request chains\n",
    "    if phase == \"Post-consent; Cookies Rejected\":\n",
    "        chains = result_dict[\"reject_flow\"][\"network_state\"][\"request_chains\"]\n",
    "    elif phase == \"Post-consent; Cookies Accepted\":\n",
    "        chains = result_dict[\"accept_flow\"][\"network_state\"][\"request_chains\"]\n",
    "    else:\n",
    "        chains = result_dict[\"page_landing\"][\"state\"][\"network_state\"][\"request_chains\"]\n",
    "\n",
    "    # We will build a new directed graph that uses \"collapsed\" node labels\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # A mapping from the collapsed node label -> set of raw URLs (to track how many)\n",
    "    node_map = {}\n",
    "    \n",
    "    # Process chains and build the graph\n",
    "    if collapse:\n",
    "        # Collapse and build the graph\n",
    "        for chain in chains:\n",
    "            raw_src = chain.get(\"source\", \"unknown\")\n",
    "            raw_tgt = chain.get(\"target\", \"unknown\")\n",
    "            \n",
    "            src = collapse_url(raw_src, requested_url, max_path_sections=2)\n",
    "            tgt = collapse_url(raw_tgt, requested_url, max_path_sections=2)\n",
    "            \n",
    "            node_map.setdefault(src, set()).add(raw_src)\n",
    "            node_map.setdefault(tgt, set()).add(raw_tgt)\n",
    "            \n",
    "            G.add_edge(src, tgt, type=chain.get(\"type\", \"unknown\"))\n",
    "    else:\n",
    "        for chain in chains:\n",
    "            src = chain.get(\"source\", \"unknown\")\n",
    "            tgt = chain.get(\"target\", \"unknown\")\n",
    "            \n",
    "            # Even when not collapsing URLs, we still want to consolidate the requested_url\n",
    "            if normalize_url(src).startswith(normalize_url(requested_url)):\n",
    "                src = requested_url\n",
    "            if normalize_url(tgt).startswith(normalize_url(requested_url)):\n",
    "                tgt = requested_url\n",
    "                \n",
    "            # When not collapsing, each node represents exactly one URL\n",
    "            node_map.setdefault(src, set()).add(src)\n",
    "            node_map.setdefault(tgt, set()).add(tgt)\n",
    "            G.add_edge(src, tgt, type=chain.get(\"type\", \"unknown\"))\n",
    "    \n",
    "    # Find orphan nodes (nodes with no incoming edges except requested_url)\n",
    "    all_nodes = set(G.nodes())\n",
    "    nodes_with_incoming = {v for u, v in G.edges()}\n",
    "    orphan_nodes = all_nodes - nodes_with_incoming - {requested_url}\n",
    "    \n",
    "    # Add edges from requested_url to orphan nodes\n",
    "    for orphan in orphan_nodes:\n",
    "        G.add_edge(requested_url, orphan, type=\"unknown\")\n",
    "    \n",
    "    # Build labels and node colors\n",
    "    labels = {}\n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        raw_count = len(node_map[node])  # Distinct raw URLs\n",
    "        if make_url_short:\n",
    "            short_label = f\"{shorten_url(node)} ({raw_count})\"  # e.g. \"abc.com/page (3)\"\n",
    "            labels[node] = short_label\n",
    "        else:\n",
    "            labels[node] = f\"{node} ({raw_count})\"\n",
    "        color = get_node_color(node)\n",
    "        node_colors.append(color)\n",
    "    \n",
    "    # Choose a layout\n",
    "    if hierarchical:\n",
    "        # Requires graphviz installed\n",
    "        # rankdir=TB means top-to-bottom flow\n",
    "        pos = graphviz_layout(G, prog='dot', args='-Grankdir=TB')\n",
    "    else:\n",
    "        # Default: force-directed layout\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Draw\n",
    "    plt.figure(figsize=(24, 16))\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=500)\n",
    "    nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20, edge_color='gray')\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)\n",
    "    \n",
    "    layout_name = \"Hierarchical (Top-Down)\" if hierarchical else \"Spring\"\n",
    "    plt.title(f\"Network Request Chains for {requested_url} - {phase} phase\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def debug_network_data(result, phase=\"Pre-consent\"):\n",
    "    \n",
    "    import json\n",
    "    from IPython.display import display, HTML\n",
    "    \n",
    "    # Get your data structure\n",
    "    data = visualisation.prepare_data_for_d3_tree(result, phase)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Root node: {data['name']}\")\n",
    "    print(f\"Number of direct children: {len(data.get('children', []))}\")\n",
    "    \n",
    "    # Show first level in formatted HTML\n",
    "    children_preview = json.dumps([\n",
    "        {\"name\": child[\"name\"], \"childCount\": len(child.get(\"children\", []))}\n",
    "        for child in data.get(\"children\", [])[:5]\n",
    "    ], indent=2)\n",
    "    \n",
    "    display(HTML(f\"<pre>{children_preview}</pre>\"))\n",
    "    \n",
    "    return data\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Pre-consent Phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_network_graph(all_results[0], False, False, True, \"Pre-consent\")\n",
    "\n",
    "# Debug the data before visualization\n",
    "print(\"Debugging network data structure:\")\n",
    "#data = debug_network_data(all_results[0], \"Pre-consent\")\n",
    "data = visualisation.prepare_data_for_d3_network(all_results[0], \"Pre-consent\")\n",
    "print(f\"Root node: {data['name']}\")\n",
    "print(f\"Number of direct children: {len(data.get('children', []))}\")\n",
    "children_preview = json.dumps([{\"name\": child[\"name\"]} for child in data.get(\"children\", [])[:5]], indent=2)\n",
    "display(HTML(f\"<pre>{children_preview}</pre>\"))\n",
    "\n",
    "visualisation.save_visualization_html(all_results[0], \"network_viz.html\", \"Pre-consent\")\n",
    "print(\"Visualization saved to network_viz.html - open this file in your browser\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Pre-consent Phase (Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Robust Enhanced Visualization - Handles both object and dictionary formats\n",
    "\n",
    "\n",
    "## Enhanced Visualization for Pre-consent Phase\n",
    "# Import the enhanced visualization module\n",
    "from enhanced_visualisation import DomainNetworkVisualizer\n",
    "from IPython.display import IFrame\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "\n",
    "# Create the domain network visualizer with the provider registry\n",
    "domain_visualizer = DomainNetworkVisualizer(provider_registry)\n",
    "\n",
    "# Function to safely get nested attribute/key from mixed object/dict structure\n",
    "def safe_get(obj, *keys):\n",
    "\n",
    "    current = obj\n",
    "    for key in keys:\n",
    "        if current is None:\n",
    "            return None\n",
    "        \n",
    "        # Handle dict case\n",
    "        if isinstance(current, dict):\n",
    "            current = current.get(key, None)\n",
    "        # Handle object case\n",
    "        elif hasattr(current, key):\n",
    "            current = getattr(current, key, None)\n",
    "        else:\n",
    "            return None\n",
    "    return current\n",
    "\n",
    "# Extract data directly for visualization to avoid circular references\n",
    "try:\n",
    "    # Get result and convert to dict for exploration\n",
    "    result = all_results[0]\n",
    "    \n",
    "    # First try to convert everything to dict\n",
    "    try:\n",
    "        result_dict = asdict(result)\n",
    "        print(\"Successfully converted result to dictionary\")\n",
    "    except Exception:\n",
    "        result_dict = result if isinstance(result, dict) else {}\n",
    "        print(\"Failed to convert full result to dictionary, proceeding with manual extraction\")\n",
    "    \n",
    "    # Get the URL\n",
    "    url = None\n",
    "    if isinstance(result_dict, dict) and \"url_info\" in result_dict:\n",
    "        url = result_dict[\"url_info\"].get(\"requested_url\", None)\n",
    "    \n",
    "    # Try direct attribute access if url is still None\n",
    "    if url is None:\n",
    "        url = safe_get(result, \"url_info\", \"requested_url\")\n",
    "    \n",
    "    if url is None:\n",
    "        url = \"Unknown URL\"\n",
    "    \n",
    "    print(f\"Processing visualization for URL: {url}\")\n",
    "    \n",
    "    # Initialize empty chains\n",
    "    simple_chains = []\n",
    "    \n",
    "    # Try multiple approaches to get request chains\n",
    "    \n",
    "    # Approach 1: Navigate through dict structure\n",
    "    if isinstance(result_dict, dict):\n",
    "        # Try the expected path for pre-consent\n",
    "        if (\"page_landing\" in result_dict and \n",
    "            isinstance(result_dict[\"page_landing\"], dict) and \n",
    "            \"state\" in result_dict[\"page_landing\"] and\n",
    "            isinstance(result_dict[\"page_landing\"][\"state\"], dict) and\n",
    "            \"network_state\" in result_dict[\"page_landing\"][\"state\"]):\n",
    "            \n",
    "            network_state = result_dict[\"page_landing\"][\"state\"][\"network_state\"]\n",
    "            \n",
    "            if isinstance(network_state, dict) and \"request_chains\" in network_state:\n",
    "                simple_chains = network_state[\"request_chains\"]\n",
    "                print(f\"Found {len(simple_chains)} request chains via dict path\")\n",
    "            elif isinstance(network_state, dict) and \"requests\" in network_state:\n",
    "                # Create chains from requests\n",
    "                requests = network_state[\"requests\"]\n",
    "                print(f\"Found {len(requests)} requests to create chains from (dict)\")\n",
    "                for req in requests:\n",
    "                    if isinstance(req, dict) and \"url\" in req:\n",
    "                        simple_chains.append({\n",
    "                            \"source\": url,\n",
    "                            \"target\": req[\"url\"],\n",
    "                            \"timestamp\": req.get(\"timestamp\", \"\")\n",
    "                        })\n",
    "    \n",
    "    # Approach 2: Direct attribute navigation\n",
    "    if not simple_chains:\n",
    "        # Try to navigate using attributes\n",
    "        network_state = safe_get(result, \"page_landing\", \"state\", \"network_state\")\n",
    "        \n",
    "        if network_state:\n",
    "            # Check if it has request_chains attribute\n",
    "            request_chains = safe_get(network_state, \"request_chains\")\n",
    "            if request_chains:\n",
    "                if isinstance(request_chains, list):\n",
    "                    simple_chains = request_chains\n",
    "                    print(f\"Found {len(simple_chains)} request chains via attribute path\")\n",
    "            \n",
    "            # If still no chains, try to build from requests\n",
    "            if not simple_chains and hasattr(network_state, \"requests\"):\n",
    "                requests = network_state.requests\n",
    "                if requests:\n",
    "                    print(f\"Found {len(requests)} requests to create chains from (attribute)\")\n",
    "                    for req in requests:\n",
    "                        if hasattr(req, \"url\") and req.url:\n",
    "                            simple_chains.append({\n",
    "                                \"source\": url,\n",
    "                                \"target\": req.url,\n",
    "                                \"timestamp\": getattr(req, \"timestamp\", \"\") if hasattr(req, \"timestamp\") else \"\"\n",
    "                            })\n",
    "    \n",
    "    # Approach 3: Direct network requests if nothing else worked\n",
    "    if not simple_chains:\n",
    "        print(\"Trying to extract network requests directly\")\n",
    "        \n",
    "        # Try to get request chains from browser manager logs\n",
    "        result_browser_state = safe_get(result, \"page_landing\", \"browser_state\")\n",
    "        if result_browser_state:\n",
    "            result_requests = safe_get(result_browser_state, \"network_requests\")\n",
    "            if result_requests and isinstance(result_requests, list):\n",
    "                print(f\"Found {len(result_requests)} direct network requests\")\n",
    "                for req in result_requests:\n",
    "                    req_url = None\n",
    "                    if isinstance(req, dict) and \"url\" in req:\n",
    "                        req_url = req[\"url\"]\n",
    "                    elif hasattr(req, \"url\"):\n",
    "                        req_url = req.url\n",
    "                    \n",
    "                    if req_url:\n",
    "                        simple_chains.append({\n",
    "                            \"source\": url,\n",
    "                            \"target\": req_url,\n",
    "                            \"timestamp\": \"\"\n",
    "                        })\n",
    "    \n",
    "    # Create minimal data structure\n",
    "    minimal_data = {\n",
    "        \"url_info\": {\"requested_url\": url},\n",
    "        \"page_landing\": {\n",
    "            \"state\": {\n",
    "                \"network_state\": {\n",
    "                    \"request_chains\": simple_chains\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Debug output\n",
    "    print(f\"Created {len(simple_chains)} request chains for visualization\")\n",
    "    \n",
    "    # If we have at least one request chain, proceed with visualization\n",
    "    if simple_chains:\n",
    "        # Save the enhanced visualization to an HTML file\n",
    "        output_file = \"enhanced_network_viz.html\" \n",
    "        success = domain_visualizer.save_domain_visualization_html(\n",
    "            minimal_data, \n",
    "            filename=output_file,\n",
    "            phase=\"Pre-consent\"\n",
    "        )\n",
    "\n",
    "        # Display the visualization directly in the notebook if successful\n",
    "        if success:\n",
    "            print(f\"Enhanced visualization saved to {output_file}\")\n",
    "            display(IFrame(src=output_file, width='100%', height=800))\n",
    "        else:\n",
    "            print(\"Failed to save visualization\")\n",
    "    else:\n",
    "        print(\"No request chains found to visualize\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error creating visualization: {e}\")\n",
    "    traceback.print_exc()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fixed Enhanced Visualization - Properly handles circular references\n",
    "\"\"\"\n",
    "\n",
    "## Enhanced Visualization for Pre-consent Phase\n",
    "# Import the fixed visualization module\n",
    "from fixed_visualizer import FixedDomainNetworkVisualizer\n",
    "from IPython.display import IFrame\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "\n",
    "# Create the domain network visualizer with the provider registry\n",
    "domain_visualizer = FixedDomainNetworkVisualizer(provider_registry)\n",
    "\n",
    "# Function to safely get nested attribute/key from mixed object/dict structure\n",
    "def safe_get(obj, *keys):\n",
    "    \"\"\"Safely get nested attribute/key from object or dictionary\"\"\"\n",
    "    current = obj\n",
    "    for key in keys:\n",
    "        if current is None:\n",
    "            return None\n",
    "        \n",
    "        # Handle dict case\n",
    "        if isinstance(current, dict):\n",
    "            current = current.get(key, None)\n",
    "        # Handle object case\n",
    "        elif hasattr(current, key):\n",
    "            current = getattr(current, key, None)\n",
    "        else:\n",
    "            return None\n",
    "    return current\n",
    "\n",
    "# Extract data directly for visualization to avoid circular references\n",
    "try:\n",
    "    # Get result and convert to dict for exploration\n",
    "    result = all_results[0]\n",
    "    \n",
    "    # First try to convert everything to dict\n",
    "    try:\n",
    "        result_dict = asdict(result)\n",
    "        print(\"Successfully converted result to dictionary\")\n",
    "    except Exception:\n",
    "        result_dict = result if isinstance(result, dict) else {}\n",
    "        print(\"Failed to convert full result to dictionary, proceeding with manual extraction\")\n",
    "    \n",
    "    # Get the URL\n",
    "    url = None\n",
    "    if isinstance(result_dict, dict) and \"url_info\" in result_dict:\n",
    "        url = result_dict[\"url_info\"].get(\"requested_url\", None)\n",
    "    \n",
    "    # Try direct attribute access if url is still None\n",
    "    if url is None:\n",
    "        url = safe_get(result, \"url_info\", \"requested_url\")\n",
    "    \n",
    "    if url is None:\n",
    "        url = \"Unknown URL\"\n",
    "    \n",
    "    print(f\"Processing visualization for URL: {url}\")\n",
    "    \n",
    "    # Save the enhanced visualization to an HTML file\n",
    "    output_file = \"enhanced_network_viz.html\" \n",
    "    success = domain_visualizer.save_domain_visualization_html(\n",
    "        result, \n",
    "        filename=output_file,\n",
    "        phase=\"Pre-consent\"\n",
    "    )\n",
    "\n",
    "    # Display the visualization directly in the notebook if successful\n",
    "    if success:\n",
    "        print(f\"Enhanced visualization saved to {output_file}\")\n",
    "        display(IFrame(src=output_file, width='100%', height=800))\n",
    "    else:\n",
    "        print(\"Failed to save visualization\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error creating visualization: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Post-consent On Accept Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_network_graph(all_results[0], False, False, True, \"Post-consent; Cookies Accepted\")\n",
    "\n",
    "# Visualize Post-consent Accept Phase with D3.js\n",
    "#display(visualisation.display_network_visualization(all_results[0], \"Post-consent; Cookies Accepted\"))\n",
    "#visualisation.save_visualization_html(all_results[0], \"network_viz.html\", \"Post-consent; Cookies Accepted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Post-consent On Reject Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_network_graph(all_results[0], False, False, True, \"Post-consent; Cookies Rejected\")\n",
    "\n",
    "# Visualize Post-consent Reject Phase with D3.js\n",
    "#display(visualisation.display_network_visualization(all_results[0], \"Post-consent; Cookies Rejected\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCleaning up resources...\")\n",
    "browser_manager.cleanup()\n",
    "print(\"Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
