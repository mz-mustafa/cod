{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cookie Consent Management Solution Test\n",
    "#### This notebook demonstrates the complete flow of checking cookie consent management on websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from url_processor import URLProcessor\n",
    "from provider_registry import ProviderRegistry\n",
    "from browser_manager import BrowserManager\n",
    "from data_collection import DataCollectionService\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import graphviz_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_processor = URLProcessor()\n",
    "provider_registry = ProviderRegistry()\n",
    "browser_manager = BrowserManager(provider_registry)\n",
    "data_collector = DataCollectionService(browser_manager)\n",
    "print(\"All components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get URLs from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"Choose URL input method:\")\n",
    "print(\"1. Enter comma-separated URLs\")\n",
    "print(\"2. Use default Abbott URLs\")\n",
    "choice = input(\"Enter choice (1 or 2): \")\n",
    "\n",
    "if choice == \"1\":\n",
    "   urls_input = input(\"Enter URLs (comma-separated): \")\n",
    "   test_urls = [url.strip() for url in urls_input.split(\",\")]\n",
    "else:\n",
    "   test_urls = [\n",
    "       \"it.pediasure.abbott\",\n",
    "       \"it.ensure.abbott\", \n",
    "       \"es.ensure.abbott\",\n",
    "       \"es.pediasure.abbott\"\n",
    "   ]\n",
    "\n",
    "# Process URLs to ensure proper format\n",
    "formatted_urls = []\n",
    "for url in test_urls:\n",
    "   if not url.startswith(('http://', 'https://')):\n",
    "       url = 'https://' + url\n",
    "   formatted_urls.append(url)\n",
    "\n",
    "print(f\"\\nProcessing {len(formatted_urls)} URLs:\")\n",
    "for url in formatted_urls:\n",
    "   print(f\"- {url}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure URL format is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = ['it.pediasure.abbott']\n",
    "formatted_urls = []\n",
    "for url in test_urls:\n",
    "   if not url.startswith(('http://', 'https://')):\n",
    "       url = 'https://' + url\n",
    "   formatted_urls.append(url)\n",
    "\n",
    "print(f\"\\nProcessing {len(formatted_urls)} URLs:\")\n",
    "for url in formatted_urls:\n",
    "   print(f\"- {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting URL processing...\")\n",
    "url_results = url_processor.process_urls(formatted_urls)\n",
    "print(f\"URL processing complete. {len(url_results)} results obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect consent data for each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for i, url_result in enumerate(url_results, 1):\n",
    "    print(\"Processing URL\")\n",
    "    if url_result.is_valid:\n",
    "        print(\"URL is valid, checking for cookie consent...\")\n",
    "        result = data_collector.create_result(url_result)\n",
    "        all_results.append(result)\n",
    "        print(\"Cookie consent check complete\")\n",
    "    else:\n",
    "        print(f\"Skipping invalid URL. Error: {url_result.error_message}\")\n",
    "    \n",
    "print(f\"Processed {len(all_results)} valid URLs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nGenerating results...\")\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"\\n=== Result {i}/{len(all_results)} ===\")\n",
    "    print(f\"URL: {result.url_info['requested_url']}\")\n",
    "    \n",
    "    #print(\"\\nFull Result:\")\n",
    "    #print(json.dumps(asdict(result), indent=2))\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    summary = data_collector.generate_cod_results(result, include_network_chains=False)\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    \n",
    "    if result.errors:\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        for error in result.errors:\n",
    "            print(f\"- {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes URLs by removing 'http://', 'https://', and 'www.' prefixes.\n",
    "    For example: 'https://www.example.com' becomes 'example.com'\n",
    "    \"\"\"\n",
    "    # Remove http:// or https://\n",
    "    if \"://\" in url:\n",
    "        url = url.split(\"://\", 1)[1]\n",
    "    \n",
    "    # Remove www.\n",
    "    if url.startswith(\"www.\"):\n",
    "        url = url[4:]\n",
    "        \n",
    "    return url\n",
    "\n",
    "def shorten_url(url):\n",
    "    \"\"\"\n",
    "    For example, 'https://abc.com/page/something' becomes 'abc.com/page'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First normalize the URL\n",
    "        url = normalize_url(url)\n",
    "        parsed = urlparse(f\"http://{url}\")  # Add scheme to make urlparse work correctly\n",
    "        netloc = parsed.netloc\n",
    "        # Process the path: strip leading/trailing slashes and split by '/'\n",
    "        if parsed.path and parsed.path != \"/\":\n",
    "            parts = parsed.path.strip(\"/\").split(\"/\")\n",
    "            if parts:\n",
    "                return f\"{netloc}/{parts[0]}\"\n",
    "        return netloc\n",
    "    except Exception:\n",
    "        # In case of any parsing error, return the original URL.\n",
    "        return url\n",
    "    \n",
    "def collapse_url(url: str, requested_url: str, max_path_sections: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Collapses URLs, with special handling for the requested_url:\n",
    "    - If URL starts with requested_url, return requested_url\n",
    "    - Otherwise collapse normally\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Normalize both URLs before comparison\n",
    "        normalized_url = normalize_url(url)\n",
    "        normalized_requested = normalize_url(requested_url)\n",
    "        \n",
    "        # Check if this URL is or starts with the requested_url\n",
    "        if normalized_url.startswith(normalized_requested):\n",
    "            return requested_url\n",
    "            \n",
    "        parsed = urlparse(f\"http://{normalized_url}\")  # Add scheme to make urlparse work correctly\n",
    "        netloc = parsed.netloc or \"unknown\"\n",
    "        # Split path into segments, ignoring empty parts\n",
    "        path_parts = [p for p in parsed.path.strip(\"/\").split(\"/\") if p]\n",
    "        # Keep only up to 'max_path_sections' segments\n",
    "        collapsed = \"/\".join(path_parts[:max_path_sections])\n",
    "        return f\"{netloc}/{collapsed}\" if collapsed else netloc\n",
    "    except Exception:\n",
    "        return url\n",
    "\n",
    "def get_node_color(short_label: str) -> str:\n",
    "    \"\"\"Assigns colors to nodes based on their domain.\"\"\"\n",
    "    lower_label = short_label.lower()\n",
    "    \n",
    "    # Mapping from keyword substring -> color\n",
    "    keyword_to_color = {\n",
    "        \"facebook\": \"blue\",\n",
    "        \"amazon\": \"orange\",\n",
    "        \"tiktok\": \"black\",\n",
    "        \"abbott\": \"royalblue\",\n",
    "        \"hubspot\": \"orange\",\n",
    "        \"hs-analytics\": \"orange\",\n",
    "        \"google\": \"yellow\",\n",
    "        \"googletagmanager\": \"yellow\",\n",
    "        \"doubleclick\": \"yellow\",\n",
    "        \"trustarc\": \"green\",\n",
    "    }\n",
    "    \n",
    "    # Return the first match or default to gray\n",
    "    for keyword, color in keyword_to_color.items():\n",
    "        if keyword in lower_label:\n",
    "            return color\n",
    "    return \"gray\"\n",
    "\n",
    "def draw_network_graph(result, hierarchical=False, collapse=False, make_url_short=False, phase=\"Pre-consent\"):\n",
    "    # Convert the dataclass to a dictionary for dictionary-style access\n",
    "    result_dict = asdict(result)\n",
    "    requested_url = result_dict[\"url_info\"][\"requested_url\"]\n",
    "\n",
    "    # Extract the request chains\n",
    "    if phase == \"Post-consent; Cookies Rejected\":\n",
    "        chains = result_dict[\"reject_flow\"][\"network_state\"][\"request_chains\"]\n",
    "    elif phase == \"Post-consent; Cookies Accepted\":\n",
    "        chains = result_dict[\"accept_flow\"][\"network_state\"][\"request_chains\"]\n",
    "    else:\n",
    "        chains = result_dict[\"page_landing\"][\"state\"][\"network_state\"][\"request_chains\"]\n",
    "\n",
    "    # We will build a new directed graph that uses \"collapsed\" node labels\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # A mapping from the collapsed node label -> set of raw URLs (to track how many)\n",
    "    node_map = {}\n",
    "    \n",
    "    # Process chains and build the graph\n",
    "    if collapse:\n",
    "        # Collapse and build the graph\n",
    "        for chain in chains:\n",
    "            raw_src = chain.get(\"source\", \"unknown\")\n",
    "            raw_tgt = chain.get(\"target\", \"unknown\")\n",
    "            \n",
    "            src = collapse_url(raw_src, requested_url, max_path_sections=2)\n",
    "            tgt = collapse_url(raw_tgt, requested_url, max_path_sections=2)\n",
    "            \n",
    "            node_map.setdefault(src, set()).add(raw_src)\n",
    "            node_map.setdefault(tgt, set()).add(raw_tgt)\n",
    "            \n",
    "            G.add_edge(src, tgt, type=chain.get(\"type\", \"unknown\"))\n",
    "    else:\n",
    "        for chain in chains:\n",
    "            src = chain.get(\"source\", \"unknown\")\n",
    "            tgt = chain.get(\"target\", \"unknown\")\n",
    "            \n",
    "            # Even when not collapsing URLs, we still want to consolidate the requested_url\n",
    "            if normalize_url(src).startswith(normalize_url(requested_url)):\n",
    "                src = requested_url\n",
    "            if normalize_url(tgt).startswith(normalize_url(requested_url)):\n",
    "                tgt = requested_url\n",
    "                \n",
    "            # When not collapsing, each node represents exactly one URL\n",
    "            node_map.setdefault(src, set()).add(src)\n",
    "            node_map.setdefault(tgt, set()).add(tgt)\n",
    "            G.add_edge(src, tgt, type=chain.get(\"type\", \"unknown\"))\n",
    "    \n",
    "    # Find orphan nodes (nodes with no incoming edges except requested_url)\n",
    "    all_nodes = set(G.nodes())\n",
    "    nodes_with_incoming = {v for u, v in G.edges()}\n",
    "    orphan_nodes = all_nodes - nodes_with_incoming - {requested_url}\n",
    "    \n",
    "    # Add edges from requested_url to orphan nodes\n",
    "    for orphan in orphan_nodes:\n",
    "        G.add_edge(requested_url, orphan, type=\"unknown\")\n",
    "    \n",
    "    # Build labels and node colors\n",
    "    labels = {}\n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        raw_count = len(node_map[node])  # Distinct raw URLs\n",
    "        if make_url_short:\n",
    "            short_label = f\"{shorten_url(node)} ({raw_count})\"  # e.g. \"abc.com/page (3)\"\n",
    "            labels[node] = short_label\n",
    "        else:\n",
    "            labels[node] = f\"{node} ({raw_count})\"\n",
    "        color = get_node_color(node)\n",
    "        node_colors.append(color)\n",
    "    \n",
    "    # Choose a layout\n",
    "    if hierarchical:\n",
    "        # Requires graphviz installed\n",
    "        # rankdir=TB means top-to-bottom flow\n",
    "        pos = graphviz_layout(G, prog='dot', args='-Grankdir=TB')\n",
    "    else:\n",
    "        # Default: force-directed layout\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Draw\n",
    "    plt.figure(figsize=(24, 16))\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=500)\n",
    "    nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20, edge_color='gray')\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)\n",
    "    \n",
    "    layout_name = \"Hierarchical (Top-Down)\" if hierarchical else \"Spring\"\n",
    "    plt.title(f\"Network Request Chains for {requested_url} - {phase} phase\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Pre-consent Phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_network_graph(all_results[0], False, False, True, \"Pre-consent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Post-consent On Accept Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_network_graph(all_results[0], False, False, True, \"Post-consent; Cookies Accepted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Post-consent On Reject Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_network_graph(all_results[0], False, False, True, \"Post-consent; Cookies Rejected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCleaning up resources...\")\n",
    "browser_manager.cleanup()\n",
    "print(\"Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
